Given a collection of records (called \emph{training set}), in which each record contains a set of attributes and one of them is the class of the record, it is possible to find a model for class attribute as a function of the values of other attributes.
The main goal is to assign a class as accurately as possible at the previously unseen record or new records; this records are called \emph{tests set}, with training set used to build the model and test set used to validate it.
The step for the classification tasks are:
\begin{enumerate}
	\item
	induction: learn the model by applying the training set and using one of the learning algorithm;
	\item
	deduction: after the creation of the model, the classification (model) is applied to the test set to check the model evaluation.
\end{enumerate}
There are several classification techniques and each of them is applicable on different situations.
The most important methods are:
\begin{itemize}
	\item
	Decision Tree based method;
	\item
	Rule based method;
	\item
	Memory based method;
	\item
	Neural networks;
	\item
	Na\"{i}ve Bayes and Bayesian Belief Networks;
	\item
	Support vector machines.
\end{itemize}
The evaluation of classification techniques are:
\begin{description}
	\item[Accuracy:] quality of the prediction;
	\item[Efficiency:] model building time and classification time;
	\item[Scalability:] training set size and attribute number (dimensionality of the object);
	\item[Robustness:] avoid of noises and sensibility to the missing data;
	\item[Interpretability:] model interpretability (by humans) and model compactness (properties of classes).
\end{description}
A \emph{Decision Tree} is a decision support tool that uses a tree-like graph or model of decision and their possible consequences.
Starting by a root, each node is represented by a splitting attribute and each leaf represents the class label; there could be more than one tree that fits the same data.
The building time is represented by the \emph{Induction} phase of classification; after that the \emph{Deduction} is done by checking the values of the attribute in the test set and treading the tree.
There are several algorithms that lead the learning of the model (\emph{Induction}) and one of the earliest is the \emph{Hunt's Algorithm}.
The general structure of such algorithm is this: let $D_t$ be the set of training records that reach a node $t$, the general procedure is:
\begin{enumerate}
	\item if $D_t$ contains records that belong to the same class $y_t$, then $t$ is a leaf node labelled as $y_t$;
	\item if $D_t$ is an empty set, then $t$ is a leaf node labelled by the default class $y_d$ (the default class is a class with the highest probability);
	\item if $D_t$ contains records that belong to more than one class, use an attribute test to split the data into smaller subsets.
	Recursively apply the procedure to each subset.
\end{enumerate}
The \emph{Induction} phase follows a \emph{Greedy} strategy that splits the records based on attribute test that optimises certain criterion.
The main issues are:
\begin{itemize}
	\item
	determine how to split the records (attribute test condition and best split);
	\item
	determine the stop condition.
\end{itemize}
The specification of test condition depends on attribute type that can be nominal, ordinal, and continuous, and it depends also on number of ways to split:
\begin{itemize}
	\item
	\emph{2-way split} for binary tree, where each node have at most two children;
	\item
	\emph{multi-way split} when each node can have more than two children.
\end{itemize}
Both for nominal and ordinal attributes: \emph{multi-way split} uses as many partitions as distinct values, \emph{binary split} divides values into two subsets (need to find the optimal partitioning).
For continuous attributes there are different ways of handling:
\begin{description}
	\item[Discretization] (multi-way split) to form an ordinal categorical attribute\mbox{}\\
	\begin{itemize}
		\item
		static (discretize once at the beginning),
		\item
		dynamic (ranges can be found by equal interval bucketing, equal frequency bucketing, or clustering);
	\end{itemize}
	\item[Binary decision] $A < v$ or $A \geq v$:\mbox{}\\
	\begin{itemize}
		\item
		consider all possible splits and finds the best cut,
		\item
		can be more compute intensive.
	\end{itemize}
\end{description}
To determine the best split condition for each node, a \emph{Greedy} approach is the following: nodes with homogeneous class distribution are preferred (\emph{pure partition} is the best situation).
To do that, we need a measure of \emph{node impurity}; the main measures of \emph{node impurity} are:
\begin{description}
	\item[Gini index]\mbox{}\\
	\emph{Gini index} for a given node $t$ is the following:
	$$
		GINI(t) = 1 - \sum_{j}\left[p\left(j|t\right)\right]^2
	$$
	where $p\left(j|t\right)$ is the relative frequency of class $j$ at node $t$.
	The maximum value is $(1 - \sfrac{1}{n_c})$ when records are equally distributed among all classes, implying least interesting information; the minimum value is $0$, when all records belong to one single class, implying most interesting information (pure partition).
	When a node $p$ is split into $k$ partitions (children), the quality of split is computed in this way:
	$$
		GINI_{split} = \sum_{i = 1}^{k} \frac{n_i}{n}GINI(i)
	$$
	where $n_i$ is the number of records at child $i$ and $n$ is the number of records at node $p$.
	Also in this case we have different type of splitting, based on type of attributes:
	\begin{description}
		\item[binary attributes:] splits into two partitions, effect of weighing partitions (larger and purer partitions are sought for);
		\item[categorical attributes:] for each distinct value, gather counts for each class in the dataset, use the count matrix to make decision;
		\item[continuous attributes:] this is the most critical situation because the attribute can also have a huge domain; usage of binary decision based on one value and several choices for the splitting value and each of them has a count matrix associated with it. To choose the best splitting attribute $v$, there is a simple method, but it is also computationally inefficient for a lot of repetitions of work (for each $v$ scan the database to gather count matrix and compute its \emph{Gini index}); there is also a variant that works in this way:
		\begin{enumerate}
			\item
			for each attribute, sort the attribute on values,
			\item
			linearly scan these values, each time updating the count matrix and computing \emph{Gini index},
			\item
			choose the split position that has the least \emph{Gini index}.
		\end{enumerate}		
	\end{description}
	\item[Entropy]\mbox{}\\
	\emph{Entropy} at given node $t$ is:
	$$
		Entropy(t) = - \sum_{j} p\left(p|j\right)log\left(p|jt\right)
	$$
	where $p\left(j|t\right)$ is the relative frequency of class $j$ at node $t$.
	\emph{Entropy} measures homogeneity of a node and the maximum value is $log\left(n_c\right)$ when records are equally distributed among all classes implying least information; minimum values is $0$ when all records belong to one class, implying most information.
	\emph{Entropy} based computations are similar to the \emph{GINI index} computations.
	The \emph{Information Gain} measure is based on \emph{Entropy} and follows this formula:
	$$
		GAIN_{split} = Entropy(p) - \left(\sum_{i = 1}^{k} \frac{n_i}{n} Entropy(i)\right)
	$$
	where parent node $p$ is split into $k$ partitions and $n_i$ is the number of records in partition $i$.
	\emph{Measures reduction} in \emph{Entropy} achieved because of the split.
	Choose the split that achieves most reduction (maximises the \emph{GAIN}).
	The disadvantage is that it tends to prefer splits that result in large number of partitions, each begins small but pure.
	The \emph{Gain Ratio} is definite in this way:
	$$
		GainRATIO_{split} = \frac{GAIN_{split}}{SplitINFO}
	$$
	and
	$$
		SplitINFO = -\sum_{i = 1}{k} \frac{n_i}{n}log\left(\frac{n_i}{n}\right)
	$$
	where parent node $p$ is split into $k$ partitions and $n_i$ is the number of records in partition $i$.
	\emph{Gain Ratio} adjust \emph{Information Gain} by the entropy of the partition (\emph{SplitINFO}); higher entropy partitioning (large number of small partitions is penalized).
	This formula is designed to overcome the disadvantage of \emph{Information Gain}.
	\item[Misclassification error]\mbox{}\\
	\emph{Classification error} at a node $t$ is:
	$$
		Error(t) = 1 - maxP\left(i|t\right)
	$$
	It measures misclassification error made by a node.
	The maximum value is $\left(1 - \sfrac{1}{n_c}\right)$ when records are equally distributed among all classes, implying least interesting information; the minimum value is $0$, when all records belong to one class, implying most interesting information.
\end{description}
There are several possible \emph{Stopping Criteria} for the \emph{Tree Induction} and some of those are:
\begin{itemize}
	\item
	stop expanding a node when all the records belong to the same class;
	\item
	stop expanding a node when all the records have similar attribute values;
	\item
	early termination.
\end{itemize}
The advantages by using a decision tree based classification are:
\begin{itemize}
	\item
	inexpensive to construct;
	\item
	extremely fast at classifying unknown records;
	\item
	easy to interpret for small-sized trees;
	\item
	accuracy.
\end{itemize}
The main disadvantage is that accuracy may be affected by missing data.
Practical issues of \emph{Classification} are:
\begin{description}
	\item[underfitting:] we have \emph{underfitting} situation when the model is too simple, both training and test errors are large;
	\item[overfitting:] we have \emph{overfitting} situation when the model is extremely specialized for the model that it can falls in mistakes; decision boundary is distorted by noise point, introduced into the initial training set; insufficient number of training records in the region causes the decision tree to predict the test examples using other training records that are irrelevant to the classification task.
	It is possible to address the overfitting by following two different methods:
	\begin{description}
		\item[pre-prunning (early stopping rule):] stop the algorithm before it becomes a fully growth tree (forcing some impurity), if all instances belong to the same class, or if all the attribute values are the same;
		\item[post-prunning:] grow decision tree to its entirety then trim the nodes of the decision tree in a bottom-up fashion; if generalization error improves after trimming, replace sub-tree by a leaf node, class label of leaf node is determined from majority class of instances in the sub-tree;
	\end{description}
	\item[missing values:] missing values affect decision tree construction in three different ways:
	\begin{itemize}
		\item
		affects how impurity measures are computed;
		\item
		affects how to distribute instance with missing value to child nodes;
		\item
		affects how a test instance with missing value is classified;
	\end{itemize}
	\item[cost of classification];
	\item[data fragmentation:] number of instances at the leaf node could be too small to take any statistically significant decision;
	\item[search strategy:] finding an optimal decision tree in a \emph{NP-Hard} problem; the algorithm presented so far uses a greedy, top-down, recursive partitioning strategy to induce a reasonable solution;
	\item[expressiveness];
	\item[tree replication:] it is possible that in a decision tree, some sub-trees appear in multiple branches.
\end{description}

Alternative techniques for classification are:
\begin{description}
	\item[Rule-Based classifier:] it is based on classify records by using a collection of ``if \ldots then \ldots'' rules, formally written in this way: $\left(Condition\right) \to y$ where \emph{Condition} is a conjunction of attributes and $y$ is the class label.
	A rule $r$ covers an instance $i$ if the attributes of the instance satisfy the condition of the rule.
	The characteristics of the \emph{Rule-Based classifier} are:
	\begin{description}
		\item[mutually exclusive rules:] classifier contains mutually exclusive rules if the rules are independent from each other and every record is covered by at most one rule;
		\item[exhaustive rules:] the classifier has exhaustive coverage if it accounts for every possible combination of attribute values and each record is covered by at least one rule.
	\end{description}
	It is possible to extract rules from a decision tree, where rules are mutually exclusive and exhaustive.
	Rule set contains as much information as the tree and for this reason rules can be simplified, but in this way they are no longer mutually exclusive because a record may trigger more than one rule (a solution is based on ordered rule set and assign the first rule to a record), also, rules are no longer exhaustive and a record may not trigger any rules (a solution is based on usage of a default class).
	Building classification rules can be done by using a direct method (extract rules directly from data) or by indirect method (extract rules from other classification models, e.g., decision tree).
	The advantages of \emph{Rule-Based} classifier are:
	\begin{itemize}
		\item
		as highly expressive as decision trees,
		\item
		easy to interpret,
		\item
		easy to generate,
		\item
		can classify new instances rapidly,
		\item
		performance comparable to decision tree.
	\end{itemize}
	\item[associative classification:] this classification model is defined by means of association rules $\left(Condition\right) \to y$ where rule body is an itemset.
	The model generation is based on rule selection and sorting (based on support, confidence, and correlation thresholds) and rule pruning (database coverage).
	The strong points of associative classification are:
	\begin{itemize}
		\item
		interpretable model,
		\item
		higher accuracy than decision tree,
		\item
		efficient classification,
		\item
		unaffected by missing data and good scalability in the training set size.
	\end{itemize}
	The weak points are:
	\begin{itemize}
		\item
		rule generation may be slow (it depends on support threshold),
		\item
		reduced scalability in the number of attributes (rule generation may become unfeasible).
	\end{itemize}
	\item[Bayesian classification:] it is based on Bayes theorem; let the class attribute and all data attributes be random variables: $C = $ any class label, $X = <x_1, \ldots, x_n>$ record to be classified.
	\emph{Bayesian classification} is based on computation of $P\left(C|x\right)$ for all classes (probably that record $x$ belongs to $C$) and assign $x$ to the class with maximal $P\left(C|x\right)$.
	Applying Bayes theorem $P\left(C|X\right) = P\left(C|X\right) * \sfrac{P\left(C\right)}{P\left(X\right)}$, $P\left(X\right)$ constant for all $C$, disregarded for maximum computation, $P\left(C\right)$ is a priori probability of $C$ $\left(P\left(C\right) = \sfrac{N_c}{N}\right)$.
	How to estimate $P\left(X|C\right)$:
	\begin{enumerate}
		\item
		Na\"{i}ve hypothesis at the base of this classification (statistical independence of attributes $x_1, \ldots, x_n$) that actually it is not always true;
		\item
		computing $P\left(x_k|C\right)$:
		\begin{itemize}
			\item
			for discrete attribute by using $P\left(x_k|C\right) = \sfrac{\left|x_k\right|}{N_c}$,
			\item
			for continuous attributes by using probability distribution
		\end{itemize}
	\end{enumerate}
	Strong points for Bayesian classification are:
	\begin{itemize}
		\item
		efficient classification,
		\item
		medium model interpretability,
		\item
		robust to isolated noise points and irrelevant attributes,
		\item
		incremental model update.
	\end{itemize}
	Weak points are:
	\begin{itemize}
		\item
		model generation without Na\"{i}ve hypothesis is unfeasible and Na\"{i}ve hypothesis may significantly reduce accuracy.
	\end{itemize}
	\item[Neural Networks:] they are inspired to the structure of the human brain (neurons as elaboration units, synapses as connection network).
	The neural network is built in this way:
	\begin{enumerate}
		\item
		for each node, definition of set of weights and offset values, providing the highest accuracy on the training data;
		\item
		iterative approach on training data instances.
	\end{enumerate}
	The base algorithm follows these steps:
	\begin{enumerate}
		\item
		initially assign random values to weights and offset;
		\item
		process instances in the training set one at a time:
		\begin{enumerate}
			\item
			for each neuron, compute the result when applying weights, offset, and activation function for the instance,
			\item
			forward propagation until the output is computed,
			\item
			compare the computed output with the expected output and evaluate errors,
			\item
			backpropagation of the error, by updating weights and offset for each neuron;
		\end{enumerate}
		\item
		the process ends when:
		\begin{itemize}
			\item
			\% of accuracy above a given threshold,
			\item
			\% of parameter variation (error) below a given threshold,
			\item
			the maximum number of epochs (iteration) is reached.
		\end{itemize}
	\end{enumerate}
	Strong points for neural networks are:
	\begin{itemize}
		\item
		extremely high accuracy,
		\item
		robust to noise and outliers (they works with percentages, so noise and outliers don't came out),
		\item
		supports both discrete and continuous output,
		\item
		efficient during classification.
	\end{itemize}
	Weak points are:
	\begin{itemize}
		\item
		in a very long training time (weakly scalable in training data size),
		\item
		not interpretable model (by human eyes).
	\end{itemize}
	\item[Support vector machines:] this method is based on finding a linear hyperplane (decision boundary) that will separate the data; it is possible to identify a lot of hyperplanes but the one which maximizes the margin is selected.
	We can find also the case in which the decision boundary is not linear and it is possible to solve this situation by transforming data into higher dimensional space.
	\item[Instance based classifier:] this classifier does not need a model, it exploits directly the itemsets.
	\emph{Instance based classifier} stores the training records in memory and it uses training records to predict the class table of unseen cases.
	There are two main examples of this type of classification:
	\begin{description}
		\item[rote-learner:] memorizes entire training data and performs classification only if the attributes of the record match one in the training examples (exactly);
		\item[nearest neighbour:] it uses $k$ ``closest'' points (nearest neighbour) to perform classification.
		It requires three things:
		\begin{itemize}
			\item
			the set of stored records,
			\item
			distance metric to compute distance between records and the value of $k$,
			\item
			the number of nearest neighbours to retrieve.
		\end{itemize}
		To classify an unknown record:
		\begin{enumerate}
			\item
			compute distance to other training records;
			\item
			identify $k$ nearest neighbours;
			\item
			use class labels of nearest neighbours to determine the class label on unknown record.
		\end{enumerate}
		To compute the distance between two points, the Euclidean distance is exploited.
		Choosing the value of $k$ is a very important point, because if $k$ is too small, the method is sensitive to noise points; if $k$ is too large, neighbourhood may include points from other classes.
	\end{description}
\end{description}
The model evaluation is based on different points:
\begin{itemize}
	\item
	metrics for performance evaluation; they focus on the predictive capability of a model and it is based on the \emph{confusion matrix}, where all cases for a correct / wrong prediction are reported: true positive, true negative, false positive, and false negative. Exploiting these values it is possible to compute the \emph{accuracy}, which is the most widely-used metric and is based on this formula:
	$$
		Accuracy = \frac{TP + TN}{TP + TN + FP + FN}
	$$
	Anyway this formula is not appropriate for unbalanced class label distribution and different class relevance and to avoid this is possible to exploit other parameters evaluated for each class and they are:
	$$
		Recall = \frac{\text{Number of objects correctly assigned to C}}{\text{Number of objects belonging to C}}
	$$
	and
	$$
		Precision = \frac{\text{Number of objects correctly assigned to C}}{\text{Number of objects assigned to C}}
	$$
	\item
	Methods for performance evaluation: there are two methods of estimation and both are based on partitioning labelled data in training set for modelling building and test set for model evaluation; they are:
	\begin{description}
		\item[Holdout] is the simplest method and consists on reserve $\sfrac{2}{3}$ of data for training and $\frac{1}{3}$ for testing; it is appropriate for large datasets and it may be repeated several times (\emph{Repeated Holdout});
		\item[Cross Validation] is used with medium size datasets and consists in:
		\begin{enumerate}
			\item
			partition data into $k$ disjoined subsets (called \emph{folds});
			\item
			$k$-fold train on $k - 1$ partitions, test on the remaining one;
			\item
			repeat for all folds.
		\end{enumerate}
		Reliable accuracy estimation, it is not appropriate for very large datasets.
	\end{description}
	\item\
	Methods for Model Comparison: they are based on the representation of the \emph{Receiver Operating Characteristic curve} (ROC) in which performance of each classifier is represented as a point in the \emph{ROC} curve (changing the threshold of algorithm changes the location of the point).
\end{itemize}