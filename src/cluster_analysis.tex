Cluster analysis are represented by finding groups of objects such that the objects in a group will be similar (or related) to one another and different from (or unrelated to) the objects on other groups.
A clustering is a set of clusters and each cluster represents a group of objects with similar features.
There are two main types of clustering:
\begin{description}
	\item[Partitional Clustering:] a division data objects into non-overlapping subsets (clusters) such that each data objects is in exactly one subset;
	\item[Hierarchical Clustering:] a set of nested clusters organized as hierarchical tree (exploiting of dendogram), this type of clustering can be traditional or non-traditional.
\end{description}
There are other distinctions between sets of clusters and they are:
\begin{description}
	\item[exclusive vs. non-exclusive:] in non-exclusive clustering, points may belong to multiple clusters;
	\item[fuzzy vs. non-fuzzy:] in fuzzy clustering, a point belongs to every cluster with some weight between $0$ and $1$, weights must sum to $1$ and probabilistic clustering has similar characteristics;
	\item[partial vs. complete:] in some cases, we only want to cluster some of the data (portion of);
	\item[heterogeneous vs. homogeneous:] cluster of widely different sizes, shapes, and densities.
\end{description}
There are several different types of clusters and they are:
\begin{description}
	\item[well-separated clusters:] a cluster is a set of points such that any point in a cluster is closer (or more similar) to every point in the cluster than to any point not in the cluster;
	\item[centered-based clusters:] a cluster is a set of points such that any point in a cluster is closer (or more similar) to every point in the cluster than to any point not in the cluster; this type is different from the precedent because the center of a cluster is identified by a ``centroid'', which is the average of all the points in the cluster (probably not a physical point), or a ``medoid'', which is the most ``representative'' point of a cluster (represented by a real point in the cluster);
	\item[contiguous clusters] (Nearest Neighbor or Transitive): a cluster is a set of points such that a point in a cluster is closer (or more similar) to one or more other points in the cluster than to any point not in the cluster;
	\item[density-based clusters:] a cluster is a dense region of points, which is separated by low-density regions, from other regions of high density; this type of cluster is used when the cluster are irregular or ``interwined'', and when noise and outliers are present;
	\item[property of conceptual clusters] (Shared Property): finds clusters that share some common property or represent a particular concept;
	\item[described by an objective function].
\end{description}

The main clustering algorithms are:
\begin{description}
	\item[K-Means and its variants] \hfill \\
	\emph{K-Means} clustering is based on \emph{Partitional clustering approach}, where each cluster is associated with a \emph{centroid}; each point is assigned to the cluster with the closest centroid.
	This algorithm needs in input, the number of clusters $K$ and these are the basic steps:
	\begin{enumerate}
		\item
		select $K$ points as the initial centroids;
		\item
		form $K$ clusters by assigning all points to the closest centroid;
		\item
		recompute the centroid of each cluster;
		\item
		repeat steps $(2)$, $(3)$ until the centroids don't change.
	\end{enumerate}
	Initial centroids are often chosen randomly (clusters produced vary from one run to another), and the centroid is typically the mean of the points in the cluster.
	``Closeness'' is measured by Euclidean distance, cosine similarity, correlation, or some other measures that can be applied.
	Most of the convergence happens in the first few iterations and often the stopping condition is changed to ``until relatively few points change clusters''.
	The complexity is $\mathcal{O}\left(n * K * i * d\right)$, where: $n$ is the number of points, $K$ is the number of clusters, $i$ is the number of iterations, and $d$ is the number of attributes.
	Because of the input value $K$ of the algorithm (number of clusters), this method probably will be executed more times, to find the best clustering criteria, especially when it's not possible to represent the whole dataset.
	To measure the quality of each solution, the most common measure is the \emph{Sum of Squared Error} (SSE): for each point, the error is the distance to the nearest cluster and to get SSE, we square these errors and sum them: $SSE = \sum_{i = 1}^{K} \sum_{x \in C_i} dist^2\left(m_i, x\right)$, where $x$ is a data point in cluster $C_i$, and $m_i$ is the representative point for cluster $C_i$.
	One easy way to reduce SSE is to increase $K$, the number of clusters; a good clustering with smallest $K$ can have a lower SSE than a poor clustering with higher $K$.

	As previously said, \emph{K-Means} needs some multiple runs to determine the best clustering; this is due to the individuation of the initial centroids; a possible solution is to select more than $K$ initial centroids and then select among these initial centroids (select the most widely separated).
	After that we need some \emph{post-processing} to eliminate artefacts (errors).
	Basic \emph{K-Means} algorithm can yeld empty clusters and to avoid this, there are several strategies:
	\begin{itemize}
		\item
		choose the point that mostly contributes to SSE;
		\item
		choose a point from the cluster with the highest SSE;
		\item
		if there are several empty clusters, the above can be repeated several times.
	\end{itemize}
	Other strategies are based on \emph{pre-processing} and \emph{post-processing}:
	\begin{itemize}
		\item
		\emph{pre-processing}:
		\begin{enumerate}
			\item
			normalize the data,
			\item
			eliminate outliers (difficult);
		\end{enumerate}
		\item
		\emph{post-processing}:
		\begin{enumerate}
			\item
			eliminate small clusters that may represent outliers,
			\item
			split ``loose'' clusters,\footnote{Clusters with relatively high SSE.}
			\item
			merge clusters that are ``close'' and with a relatively low SSE,
			\item
			can use these steps during the clustering process.
		\end{enumerate}
		\emph{Bisecting K-Means} algorithm is a variant of \emph{K-Means} that can produce a partitional or a hierarchical clustering; it is a locally optimization error.
		The main steps are:
		\begin{enumerate}
			\item
			initialize the list of clusters to contain the cluster with all points,
			\item
			select a cluster from the list of clusters,
			\item
			for each iteration, bisect the selected cluster using \emph{K-Means},
			\item
			at the end of all iterations, add the two clusters from the bisections with the lowest SEE to the list of clusters,
			\item
			repeat steps $(2)$, $(3)$, and $(4)$ until the list of clusters contains $K$ clusters.
		\end{enumerate}
		\emph{K-Means} has problems when clusters are of differing sizes, densities, or non-globular shapes.
		\emph{K-Means} also has problems when the data contains outliers.
	\end{itemize}
	\item[Hirearchical clustering] \hfill \\
	It produces a set of nested clusters organized as a hierarchical tree; it can be visualized as a dendrogram\footnote{A dendrogram is a tree diagram frequently used to illustrate the arrangement of the clusters produced by hierarchical clustering.}, which is a tree like diagram that records the sequences of merges or splits.
	The strengths of hierarchical clustering do not have to assume any particular number of initial clusters (any desired number of clusters can be obtained by ``cutting'' the dendrogram at the proper level).
	There are two main types of hierarchical clustering:
	\begin{description}
		\item[agglomerative:] it starts with the points as individual clusters, and at each step it merges the closest pair of clusters until only one cluster (or $k$ clusters) left;
		\item[divisive:] it starts with one, all-inclusive cluster, and at each step it splits a cluster until each cluster contains a point (or there are $k$ clusters).
	\end{description}
	Traditional hierarchical algorithms use a similarity or distance matrix to merge or split one cluster at a time.
	The most popular hierarchical clustering techniques is the agglomerative clustering algorithm and its basic algorithm is straightforward; the main steps are:
	\begin{enumerate}
		\item
		compute the proximity matrix;
		\item
		let each data point be a cluster;
		\item
		merge the two closest clusters;
		\item
		update the proximity matrix;
		\item
		repeat steps $(3)$ and $(4)$ until only a single cluster remains.
	\end{enumerate}
	The key operation is the computation of the proximity of two clusters, and there are different approaches to defining the distance between clusters, distinguish the different algorithms, these approaches are:
	\begin{description}
		\item[MIN or Single Link:] the similarity of two clusters is based on the two most similar (closest) points in the different clusters (strength: it can handle non-elliptical shapes - limitation: sensible to noise and outliers);
		\item[MAX or Complete Linkage:] the similarity of two clusters is based on the two least similar (most distant) points in the different clusters (strength: less susceptible to noise and outliers - limitation: it tends to break large clusters and biased towards globular clusters);
		\item[group average:] the proximity of two clusters is the average of pairwise proximity between points in the two clusters, it is a compromise between Single and Complete link (strength: less susceptible to noise and outliers - limitation: biased towards globular clusters);
		\item[Ward's method:] the similarity of two clusters is based on the increase in squared error when two clusters are merged (similar to group average if the distance between points is a squared distance); it is less susceptible to noise and outliers, based towards globular clusters and hierarchical analogue of \emph{K-Means} and in fact it can be used to initialise \emph{K-Means}.
	\end{description}
	Computational time and space requirements of hierarchical clustering tend to be high, in fact the space, since it uses the proximity matrix, is $\mathcal{O}\left(N^2\right)$ (where $N$ is the number of points) and computational time is $\mathcal{O}\left(N^3\right)$ in many cases (there are $N$ steps and at each step the size $N^2$ and proximity matrix must be updated and searched).
	The main problems for hierarchical clustering are:
	\begin{itemize}
		\item
		once a decision is made to combine two clusters, it cannot be undone;
		\item
		no objective function is directly minimized;
		\item
		different schemes have problems with one or more of the following: sensitivity to noise and outliers, difficult handling different sized clusters and convex shapes, breaking large clusters.
	\end{itemize}
	\item[Desity-based clustering (DBSCAN)] \hfill \\
	DBSCAN is a density based algorithm; density represents a number of points within a specified radius ($Eps$).
	Some definitions:
	\begin{itemize}
		\item
		a point is a \emph{core point} if it has more than a specified number of points ($MinPts$) within $Eps$ (these are points that are inside a cluster);
		\item
		a \emph{border point} has fewer than $MinPts$ within $Eps$, but it is in the neighbourhood of a core point;
		\item
		a \emph{noise point} is any point that is not a core point or a border point.
	\end{itemize}
	DBSCAN eliminates noise points and perform clustering on the remaining points; the pseudo-code for the algorithm is the following:
%	\begin{algorithmic}
%		This environment should be used to represent the pseudo-code of DBSCAN...
%	\end{algorithmic}
	\code{current_cluster_label = 1}; for each core point \code{cp}, if \code{cp} has no cluster label, then \code{current_cluster_label++} and label the current core point \code{cp} with cluster label \code{current_cluster_label}; for each point \code{p} in the $Eps-neighborhood$ (except $i^{th}$ the point itself), if \code{p} doesn't have a cluster label then label \code{p} with cluster label \code{current_cluster_label}.

	DBSCAN doesn't work well with varying density clusters and high-dimensional data.
\end{description}

The validation of clustering structures is the most difficult task; to evaluate ``goodness'' of the resulting clusters, some numerical measures can be exploited and they are classified into two main classes:
\begin{description}
	\item[External index:] used to measure the extent to which cluster labels match externally supplied class labels (such as entropy, purity, etc.);
	\item[Internal index:] used to measure the goodness of clustering structure without respect to external information (such as \emph{Sum of Squared Error} (SSE), cluster cohesion\footnote{Cluster cohesion measures how closely related are objects in a cluster.}, cluster separation\footnote{Cluster separation measures how distinct or well-separated a cluster is from other clusters.}, \emph{Rand-Index}, adjusted rand-index, ...)
\end{description}

The validation of clustering structures is the most difficult and frustrating part of cluster analysis; without a strong effort in this direction, cluster analysis will remain a black art accessible only to those true believers who have experience and great courage.